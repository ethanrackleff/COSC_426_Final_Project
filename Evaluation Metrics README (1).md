**Evaluation metrics README**

For our minimal-working-example, after running analysis on our evaluation output, we will receive an average accuracy and precision metric for each of our labels (in this case mathematical topics). One of these outputs will be measuring accuracy and precision on top-level-categories and the other output will be measuring accuracy and precision on more specific categories. Furthermore, we will cross-reference sub-category accuracy with their associated top-level-category. As a result, our data will show what categories BERT and GPT2 accurately and precisely identify as well as what specific topics of those categories they can accurately identify. 

For our final experiment, we will do the same as above, but we will focus on identifying *all* of the related categories to each theorem or definition. To do so, our evaluation metric will be an ordered ranking of each possible label. Given NLPScholar does not currently support evaluation in this manner, we will have to write a program to output this data, but it should not be too difficult. Our evaluation metric will be an ordered list of length 10 of the highest probability labels per text. Cross-referencing this with the correct dataset, we will weight the model getting the nth most likely label correct by 1/2^n. This metric will need a simple script to be calculated. As a result, for each text, we will get a single number to measure accuracy of text classification over a broad set of labels. This number will be averaged over all given texts. We will also average this number over the three types of inputs we have (theorems, definitions, and proofs) as well as over specific categories. Thus, we will be able to create bar graphs that give insight into what areas our models successfully categorized. This will give us a better diagnostic as to why models understand mathematics or not.